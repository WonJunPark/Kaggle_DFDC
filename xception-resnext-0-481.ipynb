{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnext & Xception Ensemble (Inference)\n",
    "\n",
    "- This kernel outputs the ensemble of the results from https://www.kaggle.com/khoongweihao/frames-per-video-viz and https://www.kaggle.com/greatgamedota/xception-binary-classifier-inference (not original, modified learning rate and epochs)\n",
    "- Frames per video at 64 (best found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorchcv==0.0.55) (1.17.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorchcv==0.0.55) (2.22.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorchcv==0.0.55) (2019.9.11)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorchcv==0.0.55) (1.24.2)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorchcv==0.0.55) (2.8)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorchcv==0.0.55) (3.0.4)\r\n",
      "Installing collected packages: pytorchcv\r\n",
      "Successfully installed pytorchcv-0.0.55\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install pytorchcv\n",
    "! pip install \"../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
    "\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "frame_h = 5\n",
    "frame_l = 5\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.3.0\n",
      "CUDA version: 10.0.130\n",
      "cuDNN version: 7603\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
    "sys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "facedet = BlazeFace().to(gpu)\n",
    "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
    "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
    "_ = facedet.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.read_video_1 import VideoReader\n",
    "from helpers.face_extract_1 import FaceExtractor\n",
    "\n",
    "frames_per_video = 64 #frame_h * frame_l\n",
    "video_reader = VideoReader()\n",
    "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn, facedet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
    "    h, w = img.shape[:2]\n",
    "    if w > h:\n",
    "        h = h * size // w\n",
    "        w = size\n",
    "    else:\n",
    "        w = w * size // h\n",
    "        h = size\n",
    "\n",
    "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def make_square_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    t = 0\n",
    "    b = size - h\n",
    "    l = 0\n",
    "    r = size - w\n",
    "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyResNeXt(models.resnet.ResNet):\n",
    "    def __init__(self, training=True):\n",
    "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
    "                                        layers=[3, 4, 6, 3], \n",
    "                                        groups=32, \n",
    "                                        width_per_group=4)\n",
    "        self.fc = nn.Linear(2048, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/kaggle/input/deepfakes-inference-demo/resnext.pth\", map_location=gpu)\n",
    "\n",
    "model = MyResNeXt().to(gpu)\n",
    "model.load_state_dict(checkpoint)\n",
    "_ = model.eval()\n",
    "\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레스넷\n",
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    # We keep the aspect ratio intact and add zero\n",
    "                    # padding if necessary.                    \n",
    "                    resized_face = isotropically_resize_image(face, input_size)\n",
    "                    resized_face = make_square_image(resized_face)\n",
    "\n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "                    \n",
    "                    # Test time augmentation: horizontal flips.\n",
    "                    # TODO: not sure yet if this helps or not\n",
    "                    #x[n] = cv2.flip(resized_face, 1)\n",
    "                    #n += 1\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=gpu).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction, then take the average.\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
    "                    \n",
    "                    #zyufpqvpyu.mp4\t\n",
    "                    result = y_pred[:n].mean().item()\n",
    "                    #if result <=0.5:\n",
    "                    #    result = result * 0.5\n",
    "                    #else:\n",
    "                    #    result = result + (result * 0.1)\n",
    "                    return result\n",
    "                    #return y_pred[:n].mean().item()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "        \n",
    "    # resnext return값 수정\n",
    "    return 0.481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def predict_on_video_set(videos, num_workers):\n",
    "    def process_file(i):\n",
    "        filename = videos[i]\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
    "        return y_pred\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        predictions = ex.map(process_file, range(len(videos)))\n",
    "\n",
    "    return list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_test = False  # you have to enable this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if speed_test:\n",
    "    start_time = time.time()\n",
    "    speedtest_videos = test_videos[:5]\n",
    "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_on_video_set(test_videos, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_resnext = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
    "submission_df_resnext.to_csv(\"submission_resnext.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.514175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.004443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.725788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.060911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.880215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>ztyvglkcsf.mp4</td>\n",
       "      <td>0.190556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>zuwwbbusgl.mp4</td>\n",
       "      <td>0.154894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>zxacihctqp.mp4</td>\n",
       "      <td>0.161928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>zyufpqvpyu.mp4</td>\n",
       "      <td>0.641655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>zzmgnglanj.mp4</td>\n",
       "      <td>0.051432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename     label\n",
       "0    aassnaulhq.mp4  0.514175\n",
       "1    aayfryxljh.mp4  0.004443\n",
       "2    acazlolrpz.mp4  0.725788\n",
       "3    adohdulfwb.mp4  0.060911\n",
       "4    ahjnxtiamx.mp4  0.880215\n",
       "..              ...       ...\n",
       "395  ztyvglkcsf.mp4  0.190556\n",
       "396  zuwwbbusgl.mp4  0.154894\n",
       "397  zxacihctqp.mp4  0.161928\n",
       "398  zyufpqvpyu.mp4  0.641655\n",
       "399  zzmgnglanj.mp4  0.051432\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df_resnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Xception Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
    "\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
    "sys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "facedet = BlazeFace().to(gpu)\n",
    "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
    "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
    "_ = facedet.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.read_video_1 import VideoReader\n",
    "from helpers.face_extract_1 import FaceExtractor\n",
    "\n",
    "frames_per_video = 64 # originally 4\n",
    "\n",
    "video_reader = VideoReader()\n",
    "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn, facedet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
    "    h, w = img.shape[:2]\n",
    "    if w > h:\n",
    "        h = h * size // w\n",
    "        w = size\n",
    "    else:\n",
    "        w = w * size // h\n",
    "        h = size\n",
    "\n",
    "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def make_square_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    t = 0\n",
    "    b = size - h\n",
    "    l = 0\n",
    "    r = size - w\n",
    "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth     model_v2.1.pth  pytorchcv-0.0.55-py2.py3-none-any.whl\r\n",
      "model_v0.pth  model_v2.pth\r\n",
      "model_v1.pth  model_v3.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/deepfake-xception-trained-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model\n",
    "model = get_model(\"xception\", pretrained=False)\n",
    "model = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Pooling, self).__init__()\n",
    "    \n",
    "    self.p1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "    self.p2 = nn.AdaptiveMaxPool2d((1,1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x1 = self.p1(x)\n",
    "    x2 = self.p2(x)\n",
    "    return (x1+x2) * 0.5\n",
    "\n",
    "model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)))\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "  def __init__(self, in_f, out_f):\n",
    "    super(Head, self).__init__()\n",
    "    \n",
    "    self.f = nn.Flatten()\n",
    "    self.l = nn.Linear(in_f, 512)\n",
    "    self.d = nn.Dropout(0.9)\n",
    "    self.o = nn.Linear(512, out_f)\n",
    "    self.b1 = nn.BatchNorm1d(in_f)\n",
    "    self.b2 = nn.BatchNorm1d(512)\n",
    "    self.r = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.f(x)\n",
    "    x = self.b1(x)\n",
    "    x = self.d(x)\n",
    "\n",
    "    x = self.l(x)\n",
    "    x = self.r(x)\n",
    "    x = self.b2(x)\n",
    "    x = self.d(x)\n",
    "\n",
    "    out = self.o(x)\n",
    "    return out\n",
    "\n",
    "class FCN(torch.nn.Module):\n",
    "  def __init__(self, base, in_f):\n",
    "    super(FCN, self).__init__()\n",
    "    self.base = base\n",
    "    self.h1 = Head(in_f, 1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.base(x)\n",
    "    return self.h1(x)\n",
    "\n",
    "net = []\n",
    "model = FCN(model, 2048)\n",
    "model = model.cuda()\n",
    "#model.load_state_dict(torch.load('../input/deepfakexceptiontrainedmodel2/model_xception.pth')) # new, updated\n",
    "#model.load_state_dict(torch.load('../input/modelxceptionfull125960/model_xceptionfull.pth')) # new, updated\n",
    "#model.load_state_dict(torch.load('../input/xception2/model_xception.pth')) # new, updated\n",
    "model.load_state_dict(torch.load('../input/xception3/model_xceptionfull.pth')) # new, updated\n",
    "\n",
    "\n",
    "net.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. > ## Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#익셉션\n",
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    # We keep the aspect ratio intact and add zero\n",
    "                    # padding if necessary.                    \n",
    "                    resized_face = isotropically_resize_image(face, input_size)\n",
    "                    resized_face = make_square_image(resized_face)\n",
    "\n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "                    \n",
    "                    # Test time augmentation: horizontal flips.\n",
    "                    # TODO: not sure yet if this helps or not\n",
    "                    #x[n] = cv2.flip(resized_face, 1)\n",
    "                    #n += 1\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=gpu).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction, then take the average.\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
    "                    \n",
    "                    #zyufpqvpyu.mp4\t\n",
    "                    result = y_pred[:n].mean().item()\n",
    "                    #if result <=0.5:\n",
    "                    #    result = result * 0.5\n",
    "                    #else:\n",
    "                    #    result = result + (result * 0.1)\n",
    "                    return result\n",
    "                    #return y_pred[:n].mean().item()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "        \n",
    "    # xception return값 수정\n",
    "    return 0.481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def predict_on_video_set(videos, num_workers):\n",
    "    def process_file(i):\n",
    "        filename = videos[i]\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
    "        return y_pred\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        predictions = ex.map(process_file, range(len(videos)))\n",
    "\n",
    "    return list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if speed_test:\n",
    "    start_time = time.time()\n",
    "    speedtest_videos = test_videos[:5]\n",
    "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 36s, sys: 1min 35s, total: 48min 11s\n",
      "Wall time: 25min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "predictions = predict_on_video_set(test_videos, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_xception = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
    "submission_df_xception.to_csv(\"submission_xception.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.514175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.004443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.725788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.060911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.880215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename     label\n",
       "0  aassnaulhq.mp4  0.514175\n",
       "1  aayfryxljh.mp4  0.004443\n",
       "2  acazlolrpz.mp4  0.725788\n",
       "3  adohdulfwb.mp4  0.060911\n",
       "4  ahjnxtiamx.mp4  0.880215"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df_resnext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.996301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.137819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.957771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.005402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.910421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename     label\n",
       "0  aassnaulhq.mp4  0.996301\n",
       "1  aayfryxljh.mp4  0.137819\n",
       "2  acazlolrpz.mp4  0.957771\n",
       "3  adohdulfwb.mp4  0.005402\n",
       "4  ahjnxtiamx.mp4  0.910421"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df_xception.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of Resnext and Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"filename\": test_videos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = 0.485\n",
    "r2 = 0.515\n",
    "#r1 = 0.333333\n",
    "#r2 = 0.333333\n",
    "#r3 = 0.333333\n",
    "total = r1 + r2\n",
    "r11 = r1/total\n",
    "r22 = r2/total\n",
    "#r33 = r3/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"label\"] = r22*submission_df_resnext[\"label\"] + r11*submission_df_xception[\"label\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([52., 38., 35., 25., 33., 15., 43., 45., 48., 66.]),\n",
       " array([0.00252054, 0.10198051, 0.20144047, 0.30090044, 0.4003604 ,\n",
       "        0.49982037, 0.59928033, 0.6987403 , 0.79820026, 0.89766023,\n",
       "        0.99712019]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(submission_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_df.to_csv(\"submission.csv\", index=False)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([53., 33., 36., 50., 53., 34., 36., 28., 34., 43.]),\n",
       " array([7.05145998e-04, 1.00104663e-01, 1.99504179e-01, 2.98903696e-01,\n",
       "        3.98303213e-01, 4.97702729e-01, 5.97102246e-01, 6.96501763e-01,\n",
       "        7.95901279e-01, 8.95300796e-01, 9.94700313e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(submission_df_resnext['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 84.,  19.,  18.,  17.,  31.,  10.,  20.,  13.,  19., 169.]),\n",
       " array([2.80414533e-04, 1.00251372e-01, 2.00222329e-01, 3.00193286e-01,\n",
       "        4.00164243e-01, 5.00135200e-01, 6.00106158e-01, 7.00077115e-01,\n",
       "        8.00048072e-01, 9.00019029e-01, 9.99989986e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEQNJREFUeJzt3X+w5XVdx/HnS1Yw1ALdi4O7SxebxUTGRubKYE6GYskPh+UPbJbRXI1pJyUztRRzJpoaZ1Arzcm0VYilMZTIZEcxI8SoRrCLP5AfEhsSXEH3Gko/nNDVd3+cL81tvbvn7Pmecy/3w/Mxw5zz/Xw/53zfn93Laz/3c77n+01VIUlq12NWuwBJ0nQZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGrVvtAgDWr19fs7Ozq12GJK0pN9100zeramZYv0dE0M/OzjI/P7/aZUjSmpLk30bp59KNJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ17hHxzVhJWk2zF3xi1Y5990VnTv0YzuglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS44YGfZJLkuxJcss+7a9NckeSW5O8Y0n7W5Ls7va9eBpFS5JGN8p59JcCfwxc9nBDkhcAW4BnVdVDSY7q2o8HtgLPBJ4K/F2S46rq+5MuXJI0mqEz+qq6Hnhgn+ZXAxdV1UNdnz1d+xbgw1X1UFV9FdgNnDTBeiVJB2ncNfrjgJ9JcmOSv0/ynK59A3Dvkn4LXZskaZWMewmEdcCRwMnAc4ArkjwNyDJ9a7k3SLId2A5wzDHHjFmGJGmYcWf0C8BHa+BzwA+A9V37piX9NgL3LfcGVbWjquaqam5mZmbMMiRJw4wb9B8DXgiQ5DjgUOCbwC5ga5LDkhwLbAY+N4lCJUnjGbp0k+Ry4BRgfZIF4ELgEuCS7pTL7wLbqqqAW5NcAdwG7AXO94wbSVpdQ4O+qs7dz66X76f/24C39SlKkjQ5fjNWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4oUGf5JIke7q7Se277zeSVJL13XaSvCfJ7iQ3JzlxGkVLkkY3yoz+UuC0fRuTbAJ+DrhnSfPpDO4TuxnYDryvf4mSpD6GBn1VXQ88sMyudwFvAmpJ2xbgshq4ATgiydETqVSSNJax1uiTnAV8raq+tM+uDcC9S7YXurbl3mN7kvkk84uLi+OUIUkawUEHfZLDgbcCv73c7mXaapk2qmpHVc1V1dzMzMzBliFJGtG6MV7zE8CxwJeSAGwEPp/kJAYz+E1L+m4E7utbpCRpfAc9o6+qL1fVUVU1W1WzDML9xKr6OrALeEV39s3JwINVdf9kS5YkHYxRTq+8HPgs8PQkC0nOO0D3q4G7gN3AB4DXTKRKSdLYhi7dVNW5Q/bPLnlewPn9y5IkTYrfjJWkxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxo9x45JIke5LcsqTtnUm+kuTmJH+d5Igl+96SZHeSO5K8eFqFS5JGM8qM/lLgtH3argFOqKpnAf8CvAUgyfHAVuCZ3Wv+JMkhE6tWknTQhgZ9VV0PPLBP299W1d5u8wYGNwEH2AJ8uKoeqqqvMril4EkTrFeSdJAmsUb/S8Anu+cbgHuX7Fvo2iRJq6RX0Cd5K7AX+NDDTct0q/28dnuS+STzi4uLfcqQJB3A2EGfZBvwEuBl3U3BYTCD37Sk20bgvuVeX1U7qmququZmZmbGLUOSNMRYQZ/kNODNwFlV9Z0lu3YBW5McluRYYDPwuf5lSpLGtW5YhySXA6cA65MsABcyOMvmMOCaJAA3VNWvVNWtSa4AbmOwpHN+VX1/WsVLkoYbGvRVde4yzRcfoP/bgLf1KUqSNDl+M1aSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1LihQZ/kkiR7ktyypO1JSa5Jcmf3eGTXniTvSbI7yc1JTpxm8ZKk4UaZ0V8KnLZP2wXAtVW1Gbi22wY4ncF9YjcD24H3TaZMSdK4hgZ9VV0PPLBP8xZgZ/d8J3D2kvbLauAG4IgkR0+qWEnSwRt3jf4pVXU/QPd4VNe+Abh3Sb+Fru2HJNmeZD7J/OLi4phlSJKGmfSHsVmmrZbrWFU7qmququZmZmYmXIYk6WHjBv03Hl6S6R73dO0LwKYl/TYC941fniSpr3GDfhewrXu+DbhqSfsrurNvTgYefHiJR5K0OtYN65DkcuAUYH2SBeBC4CLgiiTnAfcAL+26Xw2cAewGvgO8ago1S5IOwtCgr6pz97Pr1GX6FnB+36IkSZPjN2MlqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuOGXqb4kW72gk+s2rHvvujMVTu2JI2q14w+yeuT3JrkliSXJ3lckmOT3JjkziQfSXLopIqVJB28sYM+yQbg14C5qjoBOATYCrwdeFdVbQa+BZw3iUIlSePpu0a/DviRJOuAw4H7gRcCV3b7dwJn9zyGJKmHsYO+qr4G/D6De8beDzwI3AR8u6r2dt0WgA19i5Qkja/P0s2RwBbgWOCpwOOB05fpWvt5/fYk80nmFxcXxy1DkjREn6WbFwFfrarFqvoe8FHgp4EjuqUcgI3Afcu9uKp2VNVcVc3NzMz0KEOSdCB9gv4e4OQkhycJcCpwG3AdcE7XZxtwVb8SJUl99Fmjv5HBh66fB77cvdcO4M3AG5LsBp4MXDyBOiVJY+r1hamquhC4cJ/mu4CT+ryvJGlyvASCJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxvYI+yRFJrkzylSS3J3lukicluSbJnd3jkZMqVpJ08PrO6P8I+Juq+kngp4DbgQuAa6tqM3Btty1JWiVjB32SHwWeT3dP2Kr6blV9G9gC7Oy67QTO7lukJGl8fWb0TwMWgT9L8oUkH0zyeOApVXU/QPd41HIvTrI9yXyS+cXFxR5lSJIOpE/QrwNOBN5XVc8G/puDWKapqh1VNVdVczMzMz3KkCQdSJ+gXwAWqurGbvtKBsH/jSRHA3SPe/qVKEnqY+ygr6qvA/cmeXrXdCpwG7AL2Na1bQOu6lWhJKmXdT1f/1rgQ0kOBe4CXsXgH48rkpwH3AO8tOcxJEk99Ar6qvoiMLfMrlP7vK8kaXL8ZqwkNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN6x30SQ7pbg7+8W772CQ3JrkzyUe6m5JIklbJJGb0rwNuX7L9duBdVbUZ+BZw3gSOIUkaU6+gT7IROBP4YLcd4IUMbhQOsBM4u88xJEn99J3Rvxt4E/CDbvvJwLeram+3vQBs6HkMSVIPYwd9kpcAe6rqpqXNy3St/bx+e5L5JPOLi4vjliFJGqLPjP55wFlJ7gY+zGDJ5t3AEUkevun4RuC+5V5cVTuqaq6q5mZmZnqUIUk6kLGDvqreUlUbq2oW2Ap8uqpeBlwHnNN12wZc1btKSdLYpnEe/ZuBNyTZzWDN/uIpHEOSNKJ1w7sMV1WfAT7TPb8LOGkS7ytJ6s9vxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGTeT0SmnaZi/4xKod++6Lzly1Y0uT4Ixekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa1+fm4JuSXJfk9iS3Jnld1/6kJNckubN7PHJy5UqSDlafGf1e4I1V9QzgZOD8JMcDFwDXVtVm4NpuW5K0SvrcHPz+qvp89/w/gduBDcAWYGfXbSdwdt8iJUnjm8hFzZLMAs8GbgSeUlX3w+AfgyRH7ec124HtAMccc8wkypCasloXcvMibu3p/WFskicAfwX8elX9x6ivq6odVTVXVXMzMzN9y5Ak7UevGX2SxzII+Q9V1Ue75m8kObqbzR8N7Olb5COVMy5pslbzctQtGzvokwS4GLi9qv5wya5dwDbgou7xql4V6od4bXZJB6PPjP55wC8CX07yxa7ttxgE/BVJzgPuAV7ar0Q9kjjjktaesYO+qv4RyH52nzru+0qSJstvxkpS4wx6SWqcQS9JjZvIF6YktcMP3NvjjF6SGmfQS1LjDHpJapxr9NIQrllrrXNGL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS46YW9ElOS3JHkt1JLpjWcSRJBzaVoE9yCPBe4HTgeODcJMdP41iSpAOb1oz+JGB3Vd1VVd8FPgxsmdKxJEkHMK2g3wDcu2R7oWuTJK2waV3rZrl7ydb/65BsB7Z3m/+V5I4xj7Ue+OaYr12rHPOjg2N+FMjbe435x0fpNK2gXwA2LdneCNy3tENV7QB29D1Qkvmqmuv7PmuJY350cMyPDisx5mkt3fwzsDnJsUkOBbYCu6Z0LEnSAUxlRl9Ve5P8KvAp4BDgkqq6dRrHkiQd2NSuR19VVwNXT+v9l+i9/LMGOeZHB8f86DD1MaeqhveSJK1ZXgJBkhq3ZoJ+2CUVkhyW5CPd/huTzK58lZM1wpjfkOS2JDcnuTbJSKdaPZKNeumMJOckqSRr/gyNUcac5Be6v+tbk/zFStc4aSP8bB+T5LokX+h+vs9YjTonJcklSfYkuWU/+5PkPd2fx81JTpxoAVX1iP+PwQe6/wo8DTgU+BJw/D59XgO8v3u+FfjIate9AmN+AXB49/zVj4Yxd/2eCFwP3ADMrXbdK/D3vBn4AnBkt33Uate9AmPeAby6e348cPdq191zzM8HTgRu2c/+M4BPMvgO0snAjZM8/lqZ0Y9ySYUtwM7u+ZXAqUmW++LWWjF0zFV1XVV9p9u8gcH3FdayUS+d8XvAO4D/WcnipmSUMf8y8N6q+hZAVe1Z4RonbZQxF/Cj3fMfY5/v4aw1VXU98MABumwBLquBG4Ajkhw9qeOvlaAf5ZIK/9enqvYCDwJPXpHqpuNgLyNxHoMZwVo2dMxJng1sqqqPr2RhUzTK3/NxwHFJ/inJDUlOW7HqpmOUMf8O8PIkCwzO3nvtypS2aqZ62ZipnV45YUMvqTBin7Vk5PEkeTkwB/zsVCuavgOOOcljgHcBr1ypglbAKH/P6xgs35zC4Le2f0hyQlV9e8q1TcsoYz4XuLSq/iDJc4E/78b8g+mXtyqmml9rZUY/9JIKS/skWcfg170D/ar0SDfKmEnyIuCtwFlV9dAK1TYtw8b8ROAE4DNJ7mawlrlrjX8gO+rP9lVV9b2q+ipwB4PgX6tGGfN5wBUAVfVZ4HEMroPTqpH+fx/XWgn6US6psAvY1j0/B/h0dZ9yrFFDx9wtY/wpg5Bf6+u2MGTMVfVgVa2vqtmqmmXwucRZVTW/OuVOxCg/2x9j8ME7SdYzWMq5a0WrnKxRxnwPcCpAkmcwCPrFFa1yZe0CXtGdfXMy8GBV3T+pN18TSze1n0sqJPldYL6qdgEXM/j1bjeDmfzW1au4vxHH/E7gCcBfdp8731NVZ61a0T2NOOamjDjmTwE/n+Q24PvAb1bVv69e1f2MOOY3Ah9I8noGSxivXMsTtySXM1h6W9997nAh8FiAqno/g88hzgB2A98BXjXR46/hPztJ0gjWytKNJGlMBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY37X8JEd1BfJzszAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(submission_df_xception['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
